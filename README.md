LLM-from-Scratch

  This project serves as a hands-on, educational guide to building a Large Language Model (LLM) from scratch. It breaks down the essential components of LLMs, from text preprocessing to the core mechanisms behind transformer models.

Repository Structure

1.Tokenization(step 1)
  This notebook introduces the concept of tokenization, which is a fundamental step in preprocessing text data for machine learning. It explains how raw text is split into tokens (such as words or subwords), and the importance of this process in making the text understandable for the model.


2.BEP(Byte pair encoding, step 2)

  # Byte Pair Encoding (BPE) Tokenizer (Second Step)

This is the second step of the project, where we implement the **Byte Pair Encoding (BPE)** tokenizer. It builds on the tokenization process from the first step and applies BPE to handle subword tokenization efficiently.

## ðŸ“¦ Requirements

- Python 3.7+
- `tiktoken` library

Install dependencies:

```bash
pip install tiktoken
