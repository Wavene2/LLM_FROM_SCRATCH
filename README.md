LLM-from-Scratch

  This project serves as a hands-on, educational guide to building a Large Language Model (LLM) from scratch. It breaks down the essential components of LLMs, from text preprocessing to the core mechanisms behind transformer models.

Repository Structure

1.Tokenization
  This notebook introduces the concept of tokenization, which is a fundamental step in preprocessing text data for machine learning. It explains how raw text is split into tokens (such as words or subwords), and the importance of this process in making the text understandable for the model.
